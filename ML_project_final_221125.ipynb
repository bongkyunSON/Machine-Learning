{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "a3YlQ-fTFOWw",
        "5wvA-Xzuex0j"
      ],
      "mount_file_id": "1qDfF8t6WkZ13LfyJZtXB3onYVQVRLbWg",
      "authorship_tag": "ABX9TyO8TOIJWIAKUJXsfd+zIXkW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bongkyunSON/Machine-Learning/blob/main/ML_project_final_221125.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module Import\n",
        "### Module Version\n",
        "\n",
        "<Module Version>\n",
        "\n",
        "- numpy              1.23.4\n",
        "- pandas             1.5.1\n",
        "- scikit-learn       1.1.3\n",
        "- imbalanced-learn   0.9.1\n",
        "- xgboost            1.7.1\n",
        "- lightgbm           3.3.3\n",
        "- optuna             2.10.1\n",
        "- tqdm               4.64.1\n"
      ],
      "metadata": {
        "id": "myGxCo1ZocOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_E-AiZQpEZd",
        "outputId": "d546cc6c-0416-4ab2-c85e-0be727545802"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jtlearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWZe5qDJpyx-",
        "outputId": "ba1b2c88-6fdd-484e-8e05-b7f63e88cb62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement jtlearn (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for jtlearn\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA # 차원축소\n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score, precision_score, recall_score\n",
        "\n",
        "from imblearn.under_sampling import * # 임벨런스\n",
        "from imblearn.over_sampling import * # 임벨런스\n",
        "from imblearn.combine import * # 임벨런스\n",
        "\n",
        "import os\n",
        "import copy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from patsy import dmatrices\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# from jtlearn import Preprocessing\n",
        "# from ensemble import BinaryCalssifier, Regressor"
      ],
      "metadata": {
        "id": "vIz7tTOkofuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Data"
      ],
      "metadata": {
        "id": "37jmKbTjo2ev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Data\n",
        "save_path = 'submission/'\n",
        "base_path = 'data/'\n",
        "\n",
        "train = pd.read_csv(base_path + 'train.csv')\n",
        "test = pd.read_csv(base_path + 'test.csv')\n",
        "submission = pd.read_csv(base_path + 'sample_submission.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "72P7RVoBo1ye",
        "outputId": "a54ff6b9-2f64-49d3-9d54-23e93c3493df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-cc5d6d139c3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbase_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msubmission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'sample_submission.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/train.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing\n",
        "\n",
        "1. Fill/Drop Null data\n",
        "2. Standard Scaling\n",
        "3. Ordinal Encoding"
      ],
      "metadata": {
        "id": "Wf2L02Jvq4o6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.fillna(train.mean())\n",
        "\n",
        "X = train.drop(columns=[\"ID\", \"Y_LABEL\"])\n",
        "y = train[\"Y_LABEL\"]\n",
        "test = test.drop(columns=['ID'])\n",
        "\n",
        "train_num_cols = X.drop(columns=['COMPONENT_ARBITRARY']).columns.tolist()\n",
        "test_num_cols = test.drop(columns=['COMPONENT_ARBITRARY']).columns.tolist()\n",
        "\n",
        "ss = StandardScaler()\n",
        "ss2 = StandardScaler()\n",
        "\n",
        "ss2.fit(X[test_num_cols])\n",
        "X[train_num_cols] = ss.fit_transform(X[train_num_cols])\n",
        "test[test_num_cols] = ss2.transform(test[test_num_cols])\n",
        "\n",
        "X.COMPONENT_ARBITRARY = X.COMPONENT_ARBITRARY.map({\"COMPONENT1\" : 1, \"COMPONENT2\" : 2, \"COMPONENT3\" : 3, \"COMPONENT4\" : 4})\n",
        "test.COMPONENT_ARBITRARY = test.COMPONENT_ARBITRARY.map({\"COMPONENT1\" : 1, \"COMPONENT2\" : 2, \"COMPONENT3\" : 3, \"COMPONENT4\" : 4})\n",
        "\n",
        "X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=172)\n",
        "X_test = X_test[test.columns]"
      ],
      "metadata": {
        "id": "umkOf1tLq82u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test"
      ],
      "metadata": {
        "id": "zCrP5vyFq-u_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "nBqbVGf5rBaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regression\n",
        "### Multicollinearity"
      ],
      "metadata": {
        "id": "o7wShZe8rDtw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "empty_cols = X.drop(columns=test.columns).columns\n",
        "X_reg = X[test.columns]\n",
        "\n",
        "for col in empty_cols:\n",
        "    y_reg = X[col]\n",
        "    X_reg['intercept'] = 1\n",
        "    lm = sm.OLS(y_reg, X_reg)\n",
        "    results = lm.fit()\n",
        "\n",
        "    print(f\"***************************** {col} *****************************\\n\")\n",
        "    print(results.summary())\n",
        "\n",
        "X_reg = X_reg.drop(columns=['intercept'])"
      ],
      "metadata": {
        "id": "SGRlfIIFrGaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vif = pd.DataFrame()\n",
        "vif[\"VIF Factor\"] = [variance_inflation_factor(X_reg.values, i) for i in range(len(X_reg.columns))]\n",
        "vif[\"features\"] = X_reg.columns\n",
        "vif"
      ],
      "metadata": {
        "id": "JY-PKgq_rKvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_reg"
      ],
      "metadata": {
        "id": "L5xKo2ajrMAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### regressor hyper parameters\n",
        "\n",
        "{\n",
        "\n",
        "\"metric\": , # 평가지표(default='r2_score')\n",
        "\n",
        "\"learner\": , # 학습모델['rf', 'xgb', 'lgbm'] 중 아무 조합 선택 ㄱㄱ(default=['rf', 'xgb', 'lgbm'])\n",
        "\n",
        "\"ensemble\": , # ['voting', 'stacking'](default='voting')\n",
        "\n",
        "\"learning_rate\": , # 학습률(default=0.05)\n",
        "\n",
        "\"random_state\": , # 난수 seed(default=42)\n",
        "\n",
        "\"early_stopping_rounds\": , # overfitting 방지용(default=10)\n",
        "\n",
        "\"optimize\": , # optuna 사용할지 말지 True or False 사용 ㄱㄱ(default=False)\n",
        "\n",
        "\"n_trials\": , # optuna 횟수(default=20)\n",
        "\n",
        "\"cv\": , # K-fold CV의 K(default=5)\n",
        "\n",
        "\"N\": , # voting에서 모델별 weights의 조합가지수(default=5)\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "aoIQUjxCrOu8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Find Best Regressor For Each Column"
      ],
      "metadata": {
        "id": "L1GZeaxsrTAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learners = [['rf']]\n",
        "# learners = [['rf'], ['xgb'], ['lgbm'],\n",
        "#             ['rf', 'xgb'], ['rf', 'lgbm'], ['xgb', 'lgbm'], \n",
        "#             ['rf', 'xgb', 'lgbm']]\n",
        "\n",
        "reg_dict = {}\n",
        "\n",
        "df_score_train = pd.DataFrame([[0]*len(empty_cols)]*len(learners))\n",
        "df_score_train.columns = empty_cols\n",
        "df_score_train.index = ['+'.join(learner) for learner in learners]\n",
        "\n",
        "df_score_val = copy.deepcopy(df_score_train)\n",
        "print(\"empty column length is\", len(empty_cols))\n",
        "\n",
        "for col in empty_cols:\n",
        "    reg_list = []\n",
        "    score_list = []\n",
        "    temp_score_val = []\n",
        "    print(f\"************************* {col} *************************\\n\")\n",
        "    for learner in learners:\n",
        "        print(f\"--------------- {'+'.join(learner)} ---------------\\n\")\n",
        "        reg = Regressor(learner=learner,\n",
        "                        ensemble='stacking',\n",
        "                        metric='mse',\n",
        "                        optimize=False,\n",
        "                        n_trials=10,\n",
        "                        cv=5,\n",
        "                        N=5)\n",
        "        y_reg = X[col]\n",
        "        # train-validation split\n",
        "        X_reg_train, X_reg_val, y_reg_train, y_reg_val = train_test_split(X_reg, y_reg, test_size=0.3, random_state=69)\n",
        "\n",
        "        # model fitting\n",
        "        reg.fit(X_reg_train, y_reg_train) # n_trials: optuna 조지는 정도 / cv: K-fold의 K값\n",
        "\n",
        "        # prediction\n",
        "        y_reg_train_pred = reg.predict(X_reg_train)\n",
        "        y_reg_val_pred = reg.predict(X_reg_val)\n",
        "\n",
        "        # scoring\n",
        "        score_train = reg.score(y_reg_train, y_reg_train_pred)\n",
        "        score_val = reg.score(y_reg_val, y_reg_val_pred)\n",
        "        \n",
        "        reg_list.append(reg)\n",
        "        score_list.append(score_val)\n",
        "        df_score_train.loc['+'.join(learner), col] = score_train\n",
        "        df_score_val.loc['+'.join(learner), col] = score_val\n",
        "    \n",
        "    max_idx = score_list.index(max(score_list))\n",
        "    reg_dict[col] = reg_list[max_idx]"
      ],
      "metadata": {
        "id": "KchhG8KDrSUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_score_train"
      ],
      "metadata": {
        "id": "jePRuhyXrX51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_score_val"
      ],
      "metadata": {
        "id": "WFrHGKllrY3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drop_reg_cols = []\n",
        "final_reg_cols = []\n",
        "for col in df_score_val.columns:\n",
        "    if df_score_val[col].max() < 0.5:\n",
        "        drop_reg_cols.append(col)\n",
        "    elif df_score_val[col].max() > 0.7:\n",
        "        final_reg_cols.append(col)\n",
        "\n",
        "print(drop_reg_cols)\n",
        "print(final_reg_cols)"
      ],
      "metadata": {
        "id": "0dcYmJztrbDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "회귀성능 잘 나오는 columns\n",
        "\n",
        "['FNOX', 'FOXID', 'FSO4', 'FTBN', 'S', 'SB', 'SI', 'V100']"
      ],
      "metadata": {
        "id": "m8Lm48CGrcVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X2 = X.drop(columns=drop_reg_cols)\n",
        "\n",
        "##\n",
        "X3 = X2[test.columns]\n",
        "temp_test = copy.deepcopy(test)\n",
        "X_temp_test = copy.deepcopy(X_test)\n",
        "\n",
        "for col, reg in reg_dict.items():\n",
        "    if col not in drop_reg_cols:\n",
        "        print(col)\n",
        "        X2[col] = reg.predict(X3)\n",
        "        test[col] = reg.predict(temp_test)\n",
        "        X_test[col] = reg.predict(X_temp_test)"
      ],
      "metadata": {
        "id": "H7FUElElrjW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X2"
      ],
      "metadata": {
        "id": "998wi4SMrmNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imbalanced Sampling\n",
        "\n",
        "{\n",
        "\n",
        "\"under\": {\n",
        "\n",
        "\n",
        "    'RandomUnderSampler': RandomUnderSampler,\n",
        "\n",
        "    'TomekLinks': TomekLinks,\n",
        "\n",
        "    'CondensedNearestNeighbour': CondensedNearestNeighbour, \n",
        "\n",
        "    'OneSidedSelection': OneSidedSelection,\n",
        "\n",
        "    'EditedNearestNeighbours': EditedNearestNeighbours,\n",
        "\n",
        "    'NeighbourhoodCleaningRule': NeighbourhoodCleaningRule\n",
        "\n",
        "},\n",
        "\n",
        "\"over\": {\n",
        "\n",
        "    'RandomOverSampler': RandomOverSampler,\n",
        "\n",
        "    'SMOTE': SMOTE,\n",
        "\n",
        "    'ADASYN': ADASYN,\n",
        "\n",
        "    'NeighbourhoodCleaningRule': NeighbourhoodCleaningRule\n",
        "\n",
        "},\n",
        "\n",
        "\"hybrid\": {\n",
        "\n",
        "    'SMOTEENN': SMOTEENN,\n",
        "\n",
        "    'SMOTETomek': SMOTETomek\n",
        "\n",
        "}\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "yZuP9ud7roTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sampling_group(X, y,\n",
        "                   categorical_feature: str,\n",
        "                   use_sampler: bool=True) -> dict: \n",
        "    \"\"\"\n",
        "    divide train_df to make each group df\n",
        "    return grouped df list \n",
        "    \"\"\"\n",
        "\n",
        "    # concat X2 and y2 to divide groups\n",
        "    if hasattr(y, 'name'):\n",
        "        y_col = y.name\n",
        "    else:\n",
        "        y_col = 'Y_LABEL'\n",
        "\n",
        "    concat_df: pd.DataFrame = pd.concat([X,y], axis=1)\n",
        "    group_dic = {}\n",
        "\n",
        "    for criteria in sorted(concat_df[categorical_feature].unique()): \n",
        "        print(f\"dividing my df on {criteria}\")\n",
        "        temp_df = concat_df.loc[concat_df[categorical_feature] == criteria,].drop(columns=categorical_feature)\n",
        "\n",
        "        # make grouped X, y\n",
        "        X2 = temp_df.drop(columns=[y_col])\n",
        "        y2 = temp_df[y_col]\n",
        "\n",
        "        group_dic.update({criteria: (X2, y2)})\n",
        "\n",
        "    return group_dic"
      ],
      "metadata": {
        "id": "YSqG9a8ZrtHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sampler parameter\n",
        "variable_dict = {\n",
        "    \"test_size\": 0.1, \n",
        "    \"learner\": 'rf',\n",
        "    \"objective\": \"under\",\n",
        "    \"sampler\": \"RandomUnderSampler\",\n",
        "    \"random_state_\": 42,\n",
        "    \"dimensionality\": PCA\n",
        "}\n",
        "\n",
        "balancing = Preprocessing(**variable_dict)\n",
        "\n",
        "# 샘플링 그룹핑 스플릿\n",
        "grouped_dict = balancing.sampling_group(X2, y, categorical_feature='COMPONENT_ARBITRARY')\n",
        "split_X_y_bundle = balancing.split_X_y_bundle(grouped_dict)\n",
        "print()"
      ],
      "metadata": {
        "id": "27IkEWFQrxsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_dict[1][0]"
      ],
      "metadata": {
        "id": "2ycnWZ0FrywQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test"
      ],
      "metadata": {
        "id": "0jg3lWuqrzxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dimensionality(Feature Importance)"
      ],
      "metadata": {
        "id": "hC4AwUbCr0mh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "def choose_drop_features(feature_importance_dict, threshold: int=0, draw: bool=False):\n",
        "\n",
        "    if draw == True:\n",
        "        for criteria, feature_importance in feature_importance_dict.items(): \n",
        "            plt.figure(figsize=(12, 6))\n",
        "            plt.title(f'{criteria} Feature Importances')\n",
        "            sns.barplot(x=feature_importance, y=feature_importance.index)\n",
        "            plt.show()\n",
        "                \n",
        "    drop_target_dict = {}\n",
        "    for criteria, feature_importance in feature_importance_dict.items():\n",
        "        temp_df = feature_importance.reset_index()\n",
        "        temp_df.columns = [\"name\", \"value\"]\n",
        "\n",
        "        drop_target_dict[criteria] = temp_df[temp_df.value <= threshold].name.to_list()\n",
        "\n",
        "    return drop_target_dict"
      ],
      "metadata": {
        "id": "_T9YtoPKr2Cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_dict = balancing.feature_importance_for_groups(split_X_y_bundle)\n",
        "drop_target_dict = choose_drop_features(features_dict, draw=True)\n",
        "\n",
        "print()\n",
        "print(drop_target_dict)\n",
        "print()"
      ],
      "metadata": {
        "id": "BJbWvIhKr3kO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "['FNOX', 'FOXID', 'FSO4', 'FTBN', 'S', 'SB', 'SI', 'V100']"
      ],
      "metadata": {
        "id": "fRLMDeK4NRqu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification\n",
        "\n",
        " - classifier hyper parameters\n",
        " \n",
        "{\n",
        "\n",
        "\"metric\": , # 평가지표(default='r2_score')\n",
        "\n",
        "\"learner\": , # 학습모델['rf', 'xgb', 'lgbm'] 중 아무 조합 선택 ㄱㄱ(default=['rf', 'xgb', 'lgbm'])\n",
        "\n",
        "\"ensemble\": , # ['voting', 'stacking'](default='voting')\n",
        "\n",
        "\"learning_rate\": , # 학습률(default=0.05)\n",
        "\n",
        "\"random_state\": , # 난수 seed(default=42)\n",
        "\n",
        "\"early_stopping_rounds\": , # overfitting 방지용(default=10)\n",
        "\n",
        "\"optimize\": , # optuna 사용할지 말지 True or False 사용 ㄱㄱ(default=False)\n",
        "\n",
        "\"n_trials\": , # optuna 횟수(default=20)\n",
        "\n",
        "\"cv\": , # K-fold CV의 K(default=5)\n",
        "\n",
        "\"N\": , # voting에서 모델별 weights의 조합가지수(default=5)\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "BtRBEjDaNTqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_final = pd.DataFrame()\n",
        "val_final = pd.DataFrame()\n",
        "X_y_union = pd.concat([X_test, y_test], axis=1)\n",
        "\n",
        "for criterion, drop_target_list in drop_target_dict.items():\n",
        "    X_train, X_val, y_train, y_val = split_X_y_bundle[criterion]\n",
        "    \n",
        "    X_train = X_train.drop(columns=drop_target_list)\n",
        "    X_val = X_val.drop(columns=drop_target_list)\n",
        "    \n",
        "    test_temp = test[test.COMPONENT_ARBITRARY == criterion].drop(columns=drop_target_list)\n",
        "    test_temp = test_temp.drop(columns=['COMPONENT_ARBITRARY'])\n",
        "    \n",
        "    X_test_temp = X_y_union[X_y_union.COMPONENT_ARBITRARY == criterion].drop(columns=drop_target_list)\n",
        "    X_test_temp = X_test_temp.drop(columns=['COMPONENT_ARBITRARY', 'Y_LABEL'])\n",
        "    \n",
        "    y_test_temp = X_y_union[X_y_union.COMPONENT_ARBITRARY == criterion].Y_LABEL\n",
        "    \n",
        "    # Syncronize columns order between X3 and test\n",
        "    test_temp = test_temp[X_train.columns]\n",
        "    X_test_temp = X_test_temp[X_train.columns]\n",
        "    \n",
        "    # model initializing\n",
        "    clf = BinaryCalssifier(metric='f1_score',\n",
        "                           learner=['rf'],\n",
        "                           optimize=False,\n",
        "                           n_trials=20,\n",
        "                           cv=5,\n",
        "                           N=5,\n",
        "                           random_state=12)\n",
        "    # model training\n",
        "    clf.fit(X_train, y_train) # n_trials: optuna 조지는 정도 / cv: K-fold의 K값\n",
        "\n",
        "    # prediction\n",
        "    y_train_pred = clf.predict(X_train)\n",
        "    y_val_pred = clf.predict(X_val)\n",
        "    y_test_pred = clf.predict(X_test_temp)\n",
        "    \n",
        "    # scoring\n",
        "    score_train = clf.score(y_train, y_train_pred)\n",
        "    score_val = clf.score(y_val, y_val_pred)\n",
        "    score_test = clf.score(y_test_temp, y_test_pred)\n",
        "    \n",
        "    print(f'******************** Component {criterion} ********************')\n",
        "    print(\"Train F1_score is %.4f\" % (score_train))\n",
        "    print(\"Validation F1_score is %.4f\" % (score_val))\n",
        "    print(\"Test F1_score is %.4f\" % (score_test))\n",
        "\n",
        "    # fill prediction value in test data\n",
        "    test_temp['Y_LABEL'] = clf.predict(test_temp)\n",
        "    test_final = pd.concat([test_final, test_temp], axis=0)\n",
        "\n",
        "    # fill prediction value in dummy test data\n",
        "    X_test_temp['Y_LABEL'] = y_test_pred\n",
        "    val_final = pd.concat([val_final, X_test_temp], axis=0)\n",
        "    conf_matrix_val = confusion_matrix(y_val, y_val_pred)\n",
        "    conf_matrix_test = confusion_matrix(y_test_temp, y_test_pred)\n",
        "    \n",
        "    print('-' * 53)\n",
        "    print(\"\")\n",
        "    print(conf_matrix_val)\n",
        "    print('clf precision:\\t%.4f' % (precision_score(y_val, y_val_pred)))\n",
        "    print('clf recall:\\t%.4f' % (recall_score(y_val, y_val_pred)))\n",
        "    print('clf f1_score:\\t%.4f' % (f1_score(y_val, y_val_pred)))\n",
        "    print('-' * 53)\n",
        "    print(\"\")\n",
        "    cm_display = ConfusionMatrixDisplay(confusion_matrix = conf_matrix_test,\n",
        "                                        display_labels = ['Normal', 'Anormal'])\n",
        "    cm_display.plot()\n",
        "    plt.show()\n",
        "    print('Final model precision:\\t%.4f' % (precision_score(y_test_temp, y_test_pred)))\n",
        "    print('Final model recall:\\t%.4f' % (recall_score(y_test_temp, y_test_pred)))\n",
        "    print('Final model f1_score:\\t%.4f' % (f1_score(y_test_temp, y_test_pred)))\n",
        "    print('*' * 53)\n",
        "\n",
        "test_final = test_final.sort_index()\n",
        "val_final = val_final.sort_index()\n",
        "y_pred = val_final.Y_LABEL.values"
      ],
      "metadata": {
        "id": "-BXpv4VwNSkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_final"
      ],
      "metadata": {
        "id": "xhr4v1uONiwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_final"
      ],
      "metadata": {
        "id": "ILVqPKWlNjhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confusion Matrix"
      ],
      "metadata": {
        "id": "7O-CcyThNlCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# conf_matrix = confusion_matrix(y_val, y_pred)"
      ],
      "metadata": {
        "id": "Wt53hbZsNs3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification Metrics"
      ],
      "metadata": {
        "id": "HYgVGWGNNoJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print('*' * 40)\n",
        "# print('Final model precision:\\t%.4f' % (precision_score(y_test, y_pred)))\n",
        "# print('Final model recall:\\t%.4f' % (recall_score(y_test, y_pred)))\n",
        "# print('Final model f1_score:\\t%.4f' % (f1_score(y_test, y_pred)))\n",
        "# print('*' * 40)"
      ],
      "metadata": {
        "id": "XRnASTdoNuQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make submission file"
      ],
      "metadata": {
        "id": "rjZsxpbNNn15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'submission_oil.csv' in os.listdir(save_path):\n",
        "    count = 0\n",
        "    for name in os.listdir(save_path):\n",
        "        if 'submission_oil' in name:\n",
        "            count += 1\n",
        "    filename = f\"submission_oil{count + 1}.csv\"\n",
        "else:\n",
        "    filename = 'submission_oil.csv'\n",
        "\n",
        "# Export submission file\n",
        "submission.Y_LABEL = test_final.Y_LABEL\n",
        "submission.to_csv(save_path + filename, index=False)"
      ],
      "metadata": {
        "id": "O1UgZTYvNvaO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}